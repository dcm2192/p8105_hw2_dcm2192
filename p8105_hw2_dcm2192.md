p8105_hw2_dcm2192
================
Dylan Morgan
2024-10-02

``` r
library(tidyverse)
```

This file will include answers to the questions for HW 2.

## Problem 1

*Load NYC transit subway data, clean up variable names, retain only
select variables.*

``` r
nyc_transit <- 
  read_csv(file = "./data_hw2/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |> 
  janitor::clean_names() |> 
  select(line, station_name, station_latitude, station_longitude, route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11, entrance_type, entry, vending, ada)
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

*Convert* `entry` *variable from character to logical.*

``` r
nyc_transit$entry <- ifelse(nyc_transit$entry == "YES", TRUE, FALSE)
```

The `nyc_transit` dataset contains the variables `line`, `station_name`,
`station_latitutde`, `station_longitude`, `route1` through `route11`,
`entrance_type`, `entry`, `vending` and `ada`. Thus far, the data
cleaning process has involved using the `clean_names()` function from
the `janitor` package, which converted the variable names to snake_case
and replaced special characters if they were present in the variable
name. Other parts of the data cleaning process include using the
`select()` function to keep only the variables mentioned previously, as
well as using the `ifelse()` function to help convert the character data
type of the `entry` variable to a logical data type, converting “YES”
and “NO” to TRUE and FALSE, respectively. The `nyc_transit` dataset has
1868 rows and 19 columns. The data is relatively tidy, but there does
appear to be some redundancies in the dataset where there are multiple
rows with the same data corresponding to the same precise geographic
coordinates, so the dataset could be slightly consolidated.
